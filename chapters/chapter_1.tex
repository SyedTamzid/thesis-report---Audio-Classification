%\section{Introduction}
\section{Audio Classification} 
Audio classification is an expanding field of research with several real-world applications. Recent advances in image classification, where convolutional neural networks are used to classify pictures with high accuracy and at scale, raises the issue of whether similar approaches may be applied to other domains, such as audio classification. Audio classification or sound classification is the process of analyzing audio recordings and categorizing them in a proper way. Basically, audio classification is the task of assigning a label or class to a given audio. It may be used to identify a speaker as well as recognize which command a user is issuing or the mood of a speech. Audio classification can be of multiple types and forms such as-Acoustic event detection, music classification, Natural language classification and environmental sound classification. Despite recent advances in the field of audio classification, teaching a machine to recognize a sound and classify it into several categories is a tedious process. Starting with annotated audio data is the initial step in solving audio categorization challenges. Here are several relevant datasets for various sorts of sounds. These datasets contain a huge number of audio samples, as well as a class label for each sample that indicates what sort of sound it is, depending on the problem we’re attempting to solve. The automated classification of audio is a major challenge. Several authors have proposed methods to categorize incoming audio data based on different techniques throughout the previous decade. The majority of the proposed systems incorporate two phases of processing. The first stage analyzes the incoming waveform and extracts certain features from it. The feature extraction process generally entails a significant amount of data reduction. The second stage performs a classification based on the extracted features. A variety of signal features have been proposed for general audio classification. A second important feature set which is inherited from automatic speech recognizers consists of mel-frequency cepstral coefficients (MFCC). We applied Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) along with MFCC to improve the work proficiency of our model and tried to make comparisons with other models. CNN has shown to be effective in voice classification. We’ll exhibit how to apply Deep 4 Learning techniques to the classification of audio, specially focusing on the identification of particular sounds.
\section{Problem Statement}
Nowadays in the current world, speech recognition has gained prominence and use with the rise of AI and intelligent assistants, such as Amazon Alexa, Apple Siri, Microsoft Cortana, Google assistant. Speech recognition is the ability of a machine or a program to identify words and phrases in spoken language and convert them to a machine-readable format. Speech recognition has many applications such as voice dialing, call routing, search keywords, simple data entry. Today in the current generation speech recognition is playing a major role in most of the fields such as smartphones, Tv, voice call routing, voice dialing, search keywords, simple data entry. We all know that whenever we call any customer care service there will be a virtual assistant to assist us before we reach the main person to whom we want to talk. The technique used here was Call routing which refers to the procedure of sending voice calls to a specific queue based on predetermined criteria. A call routing system is also known as an automatic call distributor (ACD). Since traditional models of customer service were based on phone support or call support as one of the primary methods of contact between customers and companies for business purposes, the procedure of sending calls to the right agent became very much important. Today, modern agents interact with customers through a variety of channels. In earlier days most of the people use to call by typing the number in the phone but nowadays voice-enabled calling is also available where people use to call anyone through their voice without typing any number in the phone, this has made easier to the people but the problem is if anyone is in such a place where there is more noise and more disturbance then voice-enabled calling may not work correctly as more than two or more voices mixes, where it will be difficult for a system to recognize our voice in such a worst environment. To overcome this best noise elimination technique should be used where it can eliminate noise up to a level. Voice enabled calling is also known as voice dialing which uses speech recognition software. In the present world, it is possible to entry some of the data in excel or word documents using our voice which enables you 6 to do hands-free data entry by dictating the text or numbers that we want to be entered in the current cell and to issue voice commands that allow you to choose menu items, dialog box options, or even toolbar buttons by simply saying their names. This saves our time and work can be done faster compared to typing work. Even for voice data entry speech recognition software has been used. When using Speech Recognition to dictate data entries, we need to keep the microphone close to our mouth and in the same position as you dictate. Depending upon the microphone quality we need to speak normally and in a low but not monotone voice, pausing only when you come to the end of a thought or the data entry for that cell and it takes time for our computer to process our speech, and therefore, depending upon the speed of your processor, it may take some time before your words appear on the Formula bar and in the current cell. This can be improved by training more and more audio data using deep learning or machine learning algorithms. As we know Google Assistant and Microsoft Cortana are widely used nowadays such as searching for information on the internet or it may search for information on the computer such as files, folders, documents, and many other things. All these are done through our voice, whatever we speak, or we tell Google Assistant and Microsoft Cortana it will search and give us a piece of information about what we require. Therefore, Speech recognition is playing a major role in Google assistant, Microsoft Cortana, and Apple Siri. Day by Day the accuracy of converting from speech to text in Google Assistant, Microsoft Cortana, and Apple Siri is increasing. Think of a situation where you want to share your feelings to someone or you want someone to entertain you in your sad times then Amazon Alexa or amazon echo can be used, whenever we speak it will listen to us and give some information like if we tell "Alexa sing a song" it will sing some song for us or if we tell "Alexa tell us some news about today" so Alexa will tell us the news about today, Amazon Alexa is speech recognition device which recognizes our speech and depending upon that it gives some output.
\section{Research Objective }
In our research, we proposed an efficient way to classify audio data of different objects using deep learning methods using neural networks. Here, we aim to prepare a deep learning model that can predict the accuracy of different models through extracting features from the Speech Command dataset. Our prior objective is to determine the wav files so that the devices whose work is based on voice can predict properly and give better results by fulfilling customer requirements. The remaining objective is as follows:
 
•	Organizing Audio library for each class.\\
•	Finding the best performance algorithms which can work more precisely on different scenarios. \\ 
•	By using different feature extraction techniques to analyze music genre identification.\\
•	Finding different audio pattern using deep neural network 
